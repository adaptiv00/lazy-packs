stack              = kafka-connect
docker_image       = dvaida/kafka-connect-voluble:5.5.1_01
runtime_folder     = {{ .topology.config.runtime_folder }}/kafka-connect

# Connect settings
tasks              = 1
group_id           = change_me
gc_settings        = -XX:+UseG1GC -XX:MetaspaceSize=96m -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80
heap_size_settings = -Xms1g -Xmx1g

# All ENV_ vars below get injected in  the service environment if there's an ENV_LAZY_PLACEHOLDER: "" in the stack file
ENV_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR = 1
ENV_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR = 1
ENV_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR = 1
ENV_CONNECT_KEY_CONVERTER = io.confluent.connect.avro.AvroConverter
ENV_CONNECT_VALUE_CONVERTER = io.confluent.connect.avro.AvroConverter
ENV_CONNECT_INTERNAL_KEY_CONVERTER = org.apache.kafka.connect.json.JsonConverter
ENV_CONNECT_INTERNAL_VALUE_CONVERTER = org.apache.kafka.connect.json.JsonConverter
ENV_CONNECT_INTERNAL_KEY_CONVERTER_SCHEMAS_ENABLE = "false"
ENV_CONNECT_INTERNAL_VALUE_CONVERTER_SCHEMAS_ENABLE = "false"

# Comma separated list of topics that will be created after deploy, if not empty e.g. lock, stock, smocking-barrels
topics_to_create       =
kafka_topic_partitions = 24
kafka_topic_replicas   = 3
